\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode`:\active
\catcode`;\active
\catcode`!\active
\catcode`?\active
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{french}
\@writefile{toc}{\select@language{french}}
\@writefile{lof}{\select@language{french}}
\@writefile{lot}{\select@language{french}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Neurone grossi 63 fois, image de Kieran Boyle}}{1}{figure.1}}
\newlabel{fig:Neurone}{{1}{1}{Neurone grossi 63 fois, image de Kieran Boyle}{figure.1}{}}
\citation{Rosenblatt}
\@writefile{toc}{\contentsline {section}{\numberline {1}R\IeC {\'e}seau de neurones}{3}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Pr\IeC {\'e}sentation}{3}{subsection.1.1}}
\citation{MinskyPapert}
\citation{Lee}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Neurones binaires et algorithme du "perceptron convergence procedure"}{5}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Neurones lin\IeC {\'e}aires ou filtres lin\IeC {\'e}aires}{6}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}La surface d'erreur du neurone lin\IeC {\'e}aire}{6}{subsubsection.1.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces La forme typique d'une surface d'erreur quadratique.}}{7}{figure.2}}
\newlabel{fig:Surface}{{2}{7}{La forme typique d'une surface d'erreur quadratique}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Les lignes de niveau d'une surface d'erreur quadratique sont bien des ellipses. }}{7}{figure.3}}
\newlabel{fig:level}{{3}{7}{Les lignes de niveau d'une surface d'erreur quadratique sont bien des ellipses}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Pourquoi plusieurs couches?}{8}{subsubsection.1.3.2}}
\newlabel{XOR}{{1.3.2}{8}{Pourquoi plusieurs couches?}{subsubsection.1.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Repr\IeC {\'e}sentation graphique du <<ou exclusif>>.}}{8}{figure.4}}
\newlabel{RN2}{{1.3.2}{8}{Pourquoi plusieurs couches?}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces R\IeC {\'e}seau de neurones \IeC {\`a} deux couches.}}{8}{figure.5}}
\newlabel{Dessin}{{1.3.2}{9}{Pourquoi plusieurs couches?}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces S\IeC {\'e}paration non lin\IeC {\'e}aire}}{9}{figure.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Exp\IeC {\'e}riences autour du Xor}{10}{subsubsection.1.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Repr\IeC {\'e}sentation d'un tirage de xor al\IeC {\'e}atoire}}{10}{figure.7}}
\newlabel{fig:xoralea}{{7}{10}{Représentation d'un tirage de xor aléatoire}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Reconnaissance par un svm \textbf  {lin\IeC {\'e}aire} du xor}}{13}{figure.8}}
\newlabel{fig:svmlinxor}{{8}{13}{Reconnaissance par un svm \textbf {linéaire} du xor}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Reconnaissance par un svm avec un noyau \textbf  {de degr\IeC {\'e} $2$} du xor}}{14}{figure.9}}
\newlabel{fig:svmpolydot}{{9}{14}{Reconnaissance par un svm avec un noyau \textbf {de degré $2$} du xor}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Reconnaissance par un r\IeC {\'e}seau neuronal du xor}}{15}{figure.10}}
\newlabel{fig:nnxor}{{10}{15}{Reconnaissance par un réseau neuronal du xor}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Neurones logistiques}{16}{subsection.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Algorithme de r\IeC {\'e}tropropagation}{16}{subsection.1.5}}
\citation{Amari}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Point de vue g\IeC {\'e}om\IeC {\'e}trique}{17}{subsection.1.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Machines de Boltzmann}{17}{subsection.1.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.1}Introduction}{17}{subsubsection.1.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.2}Dynamique stochastique d'une machine de Boltzmann}{18}{subsubsection.1.7.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.3}Learning dans les machines de Boltzmann}{19}{subsubsection.1.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Restricted Boltzmann machines}{20}{subsection.1.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.1}Introduction}{20}{subsubsection.1.8.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2}Deep Learning \IeC {\`a} l'aide de RBM}{20}{subsubsection.1.8.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.3}Pr\IeC {\'e}cisions sur l'algorithme de Contrastive divergence (not\IeC {\'e} CD-1)}{21}{subsubsection.1.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Deep Belief Networks}{22}{subsection.1.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Deep Belief Network}}{23}{figure.11}}
\newlabel{DBN}{{11}{23}{Deep Belief Network}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Multilayer Neural Network}{24}{subsection.1.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11}Cas du perceptron}{25}{subsection.1.11}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Point de vue g\IeC {\'e}om\IeC {\'e}trique en statistique}{26}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}G\IeC {\'e}om\IeC {\'e}trie de l'information}{26}{subsection.2.1}}
\citation{Amari1994}
\citation{Amari1998}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Algorithme de descente de gradient naturel}{27}{subsection.2.2}}
\newlabel{ellipse}{{2.2}{27}{Algorithme de descente de gradient naturel}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces En bleu, le gradient euclidien, et en rouge le gradient corrig\IeC {\'e}. En un point o\IeC {\`u} l'ellipse est tr\IeC {\`e}s courb\IeC {\'e}e, le gradient riemannien donne une meilleure direction de descente.}}{27}{figure.12}}
\citation{Ollivier}
\citation{Ollivier}
\citation{Ollivier}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}M\IeC {\'e}triques invariantes}{29}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}M\IeC {\'e}triques de Fisher pour les r\IeC {\'e}saux de neurones}{30}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}M\IeC {\'e}thodes d'apprentissage profond test\IeC {\'e}es sur MNIST}{30}{subsection.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Pourcentage d'erreur sur l'\IeC {\'e}chantillons test de MNIST en fonction des it\IeC {\'e}rations. Le r\IeC {\'e}seau de neurones \IeC {\`a} deux couches en bleu d\IeC {\'e}cro\IeC {\^\i }t beaucoup plus vite.}}{31}{figure.13}}
\newlabel{fig:Erreur}{{13}{31}{Pourcentage d'erreur sur l'échantillons test de MNIST en fonction des itérations. Le réseau de neurones à deux couches en bleu décroît beaucoup plus vite}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Comparaison entre descente de gradient euclidienne et riemannienne sur l'exemple du XOR al\IeC {\'e}atoire}{32}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Mod\IeC {\`e}le}{32}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Proc\IeC {\'e}dure de test}{33}{subsection.3.2}}
\bibstyle{plain}
\bibdata{biblio}
\bibcite{Cybenko1989}{1}
\bibcite{Hinton2006}{2}
\bibcite{Hornik1991}{3}
\bibcite{Amari1994}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Performances de la descente de gradient Riemanienne (en rouge) par rapport \IeC {\`a} la m\IeC {\'e}thode classique (en bleu) pour l'exemple du Xor al\IeC {\'e}atoire en terme de pourcentage d'erreurs de pr\IeC {\'e}diction de labels en fonction du temps de calcul}}{34}{figure.14}}
\newlabel{fig:xorgrad}{{14}{34}{Performances de la descente de gradient Riemanienne (en rouge) par rapport à la méthode classique (en bleu) pour l'exemple du Xor aléatoire en terme de pourcentage d'erreurs de prédiction de labels en fonction du temps de calcul}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}R\IeC {\'e}sultats}{34}{subsection.3.3}}
\bibcite{Amari1998}{5}
\bibcite{Kenny}{6}
\bibcite{Lafontaine}{7}
\bibcite{Lee}{8}
\bibcite{MinskyPapert}{9}
\bibcite{Ollivier}{10}
\bibcite{Rosenblatt}{11}
\bibcite{Amari}{12}
\citation{*}
